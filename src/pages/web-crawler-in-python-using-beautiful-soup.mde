---
title: Web crawler in python using beautiful soup
date: "2018-09-08"
description: "Making a web crawler to find articles relevant to a thesis"
---

<paragraph>
  In 2016, a friend of mine pursuing a Master's of Engineering degree from
  <a href="http://www.tsinghua.edu.cn/publish/thu2018en/index.html" target="\_blank">Tsinghua University</a> reached out to me to seek help on figuring out how to efficiently
  find articles online relevant to his thesis on to corporate social responsibility of Chinese construction
  companies working in Nigeria.
</paragraph>
<paragraph>
  I assumed that he'd need a web crawler, so we started looking into and planning a strategy
  around how to build what he needed. I had the opportunity to use HTML parsers in both Java and
  python in the past for <a href="https://en.wikipedia.org/wiki/Web_scraping" target="\_blank">web scrapers</a> (<a href="https://jsoup.org/" target="\_blank">Jsoup in Java</a>
  and <a href="https://www.digitalocean.com/community/tutorials/how-to-scrape-web-pages-with-beautiful-soup-and-python-3">
  Beautiful Soup in python</a>), but I never actually had built a <a href="https://en.wikipedia.org/wiki/Web_crawler" target="\_blank">web crawler</a> before.
</paragraph>
<paragraph>
  The internet can be modeled as a <a href="https://en.wikipedia.org/wiki/Graph_(abstract_data_type)" target="\_blank">graph data structure</a>, and we would have to start with several root urls to crawl from. We can crawl indefinitely if each web page, or node, has links to other webpages, or nodes, but specifying a crawl depth will limit how far the web crawler goes, or how deep we go in the 'graph' of web pages.
</paragraph>
<paragraph>
  He came to me with a list of keywords that he wanted to explore, so with that, we looked up a couple of base urls where we'd want to start crawling from, based from preliminary research.
</paragraph>

<paragraph>

```python

base_urls = [
  {'url':'http://www.allafrica.com','crawl_depth':20,'type':'public'},
  {'url':'http://www.imostate.gov.ng','crawl_depth':5,'type':'gov'},
  {'url':'http://www.abiastate.gov.ng','crawl_depth':5,'type':'gov'},
  {'url':'http://www.anambrastate.gov.ng','crawl_depth':5,'type':'gov'},
  {'url':'http://www.ebonyistate.gov.ng','crawl_depth':5,'type':'gov'},
  ...
]

keyword_to_explore = ['Projects','China','Chinese','Construction','Construct','Build','Contractor','Contractors','Nigeria','Projects','Infrastructure']

```

</paragraph>

<paragraph>
  For classification purposes, we differentiated 'base_urls' on wether the website is managed by a government entity or a public entity. The 'keywords_to_explore' were used to qualitatively assess the 'relevance' of the article, or how likely the article would have the content he would be interested in. We tracked whether or not the word was seen and how many times the word was seen. While there are machine learning techniques available to quantitatively classify '<a href="https://en.wikipedia.org/wiki/Relevance_(information_retrieval)" target="\_blank">relevance</a>', I didn't have the time (or significant expertise) to do it and do it quickly, given the limited amount of time I had as well as his impending deadline.
</paragraph>

<paragraph>
  <div>  From this information, we are now able to crawl in an attempt to find articles that are, hopefully, relevant. The code will operate as followed:</div>
  <br/>
  <ul>
    <li>For each 'base_url', retrieve the HTML document.</li>
    <li>Grab each anchor tag from the HTML document.</li>
    <li>For each anchor tag in document, retrieve new HTML document if that url has not been scanned.</li>
    <li>Retrieve anchor tags in new HTML document.</li>
    <li>Continue process until the 'crawl_depth' is reached for the base_url.</li>
  </ul>
</paragraph>

<paragraph>

  ```python

  import urllib2
  import httplib2
  from html import HTML
  from bs4 import BeautifulSoup
  import re
  import mysql.connector
  import os

  url_checked = []

  http = httplib2.Http()

  local_host = 'localhost';
  local_username = 'alejandro'
  local_password = '9016403'
  local_database = 'scraper'
  local_port = '3306'

  local_mysql_connection = mysql.connector.connect(host=local_host,user=local_username,database=local_database,password=local_password,port=local_port)
  local_mysql_cursor = local_mysql_connection.cursor(buffered=True)

  north_east = ['adamawa','bauchi','borno','gombe','taraba','yobe']
  north_central = ['benue','kogi','kwara','nasarawa','niger','federal capital territory','plateau']
  north_west = ['Kaduna','Kano','Kebbi','Sokoto','Zamfara','Katsina']
  south_south = ['Rivers','Edo','Akwa Ibom','Bayelsa','Cross River','Delta']
  south_west = ['Lagos','Ogun','Ondo','Osun','Oyo','Ekiti']
  south_east = ['abia','anambra','ebonyi','enugu','imo']

  keyword_array = ['Projects','China','Chinese','Construction','Construct','Build','Contractor','Contractors','Nigeria','Projects','Infrastructure']

  crawl_depth_limit = 20

  http = httplib2.Http()

  links = []
  urls = [
    {'url':'http://www.allafrica.com','crawl_depth':20,'type':'public'},
    {'url':'http://www.imostate.gov.ng','crawl_depth':5,'type':'gov'},
    {'url':'http://www.abiastate.gov.ng','crawl_depth':5,'type':'gov'},
    {'url':'http://www.anambrastate.gov.ng','crawl_depth':5,'type':'gov'},
    {'url':'http://www.ebonyistate.gov.ng','crawl_depth':5,'type':'gov'}
  ]

  for url in urls:

  	try:
  		crawl_depth = url['crawl_depth']
  		site_type = url['type']
  		url = url['url']

  		regions = []
  		keywords = []
  		html = ''

  		status, response = http.request(url)
  		soup = BeautifulSoup(response.lower(),"html.parser")

  		links = soup.find_all('a',href=True)

  		for link in links:
  			url_scan = {}
  			href = link.get('href')
  			url_scan['crawl_depth'] = crawl_depth + 1
  			url_scan['type'] = site_type
  			if href[:4] == 'http':
  				url_scan['url'] = href
  			else:
  				url_scan['url'] = url + href

  			if url_scan not in urls:
  				urls.append(url_scan)

  		ps = soup.find_all('p')

  		for p in ps:
  			html = html + p.get_text().encode('utf-8')

  		#print html

  		###Search Text###
  		for word in keyword_array:
  			word = word.lower()
  			if word in html:
  				keywords.append(word)

  		for region in north_west:
  			region = region.lower()
  			if region in html:
  				regions.append('North West')
  				keywords.append(region)

  		for region in north_central:
  			region = region.lower()
  			if region in html:
  				regions.append('North Central')
  				keywords.append(region)

  		for region in north_east:
  			region = region.lower()
  			if region in html:
  				regions.append('North East')
  				keywords.append(region)

  		for region in south_east:
  			region = region.lower()
  			if region in html:
  				regions.append('South East')
  				keywords.append(region)

  		for region in south_south:
  			region = region.lower()
  			if region in html:
  				regions.append('South South')
  				keywords.append(region)

  		for region in south_west:
  			region = region.lower()
  			if region in html:
  				regions.append('South West')
  				keywords.append(region)

  		regions = ','.join(regions)
  		keywords = ','.join(keywords)

  		###Save Text###
  		filename = re.sub(r'[^a-zA-Z0-9]','', url)
  		filename = filename[0:250]

  		if 'South West' in regions:
  			new_file = open('/all_africa_scrape/south_west/'+filename+'.txt','w')
  			new_file.write(html)

  		if 'South South' in regions:
  			new_file = open('/all_africa_scrape/south_south/'+filename+'.txt','w')
  			new_file.write(html)


  		if 'South East' in regions:
  			new_file = open('/all_africa_scrape/south_east/'+filename+'.txt','w')
  			new_file.write(html)

  		if 'North East' in regions:
  			new_file = open('/all_africa_scrape/north_east/'+filename+'.txt','w')
  			new_file.write(html)


  		if 'North West' in regions:
  			new_file = open('/all_africa_scrape/north_west/'+filename+'.txt','w')
  			new_file.write(html)


  		if 'North Central' in regions:
  			new_file = open('/all_africa_scrape/north_central/'+filename+'.txt','w')
  			new_file.write(html)


  		insert_sql = ("INSERT INTO all_africa_scrape(url,keywords,regions,scanned,filename,site_type) VALUES(%s,%s,%s,%s,%s,%s)")
  		insert_data = (url,regions,keywords,1,filename,site_type)
  		local_mysql_cursor.execute(insert_sql,insert_data)
  		local_mysql_connection.commit()
  	except:
  		print 'URL Failed'

  local_mysql_cursor.close()
  local_mysql_connection.close()
  ```

</paragraph>

<paragraph>
In the code snippet, I save the results of the scrape to a table in a MySQL
database and the HTML called for each URL into text files.
</paragraph>
